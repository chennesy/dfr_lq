{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and clean metadata and ngrams\n",
    "This notebook pulls the metadata as prepared in notebook *1_import_r.ipynb* removes unwanted items from the corpus, and then pulls in the ngrams for the articles matching on metadata ids. The notebook also recreates a bag of words from the ngram counts to enable the use of countvectorizer in notebooks 3 and 4. \n",
    "### Import required libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob \n",
    "import re\n",
    "import numpy as np\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load metadata from R notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_R = pd.read_csv('part1_journal_article_jst_get_article-1.csv', encoding='utf-8', na_filter=False)\n",
    "df_R.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial article count\n",
    "Make sure ngrams are found for each metadata row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filesystem_article_count = len(glob.glob(\"ngrams/*.txt\"))\n",
    "starting_article_count = len(df_R)\n",
    "assert starting_article_count == filesystem_article_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove non-article items from corpus\n",
    "Drop all rows with an article_title of \"Front Matter\", \"The Cover Design\", \"The Cover\", \"Back Matter\", and \"Volume Information.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_count_for_column_value(df_R, column, value):\n",
    "    return len(df_R[df_R[column] == value])\n",
    "\n",
    "def row_count_for_column_value_regex(df_R, column, regex):\n",
    "    return len(df_R[df_R[column].str.contains(regex, regex=True, na=False)])\n",
    "\n",
    "for article_title in ['Front Matter','Back Matter','Volume Information',]:\n",
    "    if row_count_for_column_value(df_R, 'article_title', article_title) == 0:\n",
    "        print(f'no articles with title \"{article_title}\"')\n",
    "        continue\n",
    "    df_R = df_R[~(df_R['article_title']==article_title)]\n",
    "    assert row_count_for_column_value(df_R, 'article_title', article_title) == 0\n",
    "    \n",
    "for article_title_regex in ['^The\\sCover','^Cover\\sDesign',]:\n",
    "    if row_count_for_column_value_regex(df_R, 'article_title', article_title_regex) == 0:\n",
    "        print(f'no articles with title \"{article_title}\"')\n",
    "        continue\n",
    "    df_R = df_R[~(df_R['article_title'].str.contains(article_title_regex, regex=True, na=False))]\n",
    "    assert row_count_for_column_value_regex(df_R, 'article_title', article_title_regex) == 0\n",
    "    \n",
    "print(\"Current article count:\", len(df_R))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix journal_pub_id values \n",
    "Move values for 'libraryq' into journal_jcode column - these were incorrectly mapped via the R import. You can manually check values with:\n",
    "```df_R['journal_pub_id'].unique()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "journal_pub_id_count = row_count_for_column_value(df_R, 'journal_pub_id', 'libraryq') \n",
    "starting_journal_jcode_count = row_count_for_column_value(df_R, 'journal_jcode', 'libraryq') \n",
    "df_R.loc[df_R['journal_pub_id']=='libraryq', 'journal_jcode'] = 'libraryq'\n",
    "assert row_count_for_column_value(df_R, 'journal_jcode', 'libraryq') == starting_journal_jcode_count + journal_pub_id_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create id column to match against ngram ids\n",
    "You can check the number of matches via:\n",
    "\n",
    "```len(df_R['file_name'].str.contains(r\"^journal-article-\", regex=True, na=False)) # 8099```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert row_count_for_column_value_regex(df_R, 'file_name', r'^journal-article-') > 0\n",
    "df_R['file_name'] = df_R['file_name'].str.replace(r'^journal-article-', '', regex=True)\n",
    "assert row_count_for_column_value_regex(df_R, 'file_name', r'^journal-article-') == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export to csv for notebook 3 combination with ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_R.to_csv('output/df-R-cleaned.csv', encoding='utf-8', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required libraries to reconstitute bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "nltk.download('stopwords')\n",
    "#from sklearn.feature_extraction.text import CountVectorizer\n",
    "#from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create bag of words from ngrams\n",
    "Recreate texts with all stemmed words (out of order) from ngram word counts to be able to use CountVectorizer for LDA in notebooks 3 and 4. Removes digits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_list = []\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "\n",
    "ngrams = glob.iglob(\"ngrams/*.txt\")\n",
    "\n",
    "for ngram in ngrams:\n",
    "    word_count_map = {}\n",
    "    #create n_id from file name\n",
    "    n_id = ngram[:-11].strip('ngrams/journal-article-') \n",
    "    with open(ngram) as csv_ng:\n",
    "        csvReader = csv.reader(csv_ng, delimiter='\\t')\n",
    "        for row in csvReader:\n",
    "            word, count = row[0], row[1]\n",
    "            if word not in word_count_map:\n",
    "                word_count_map[word] = 0\n",
    "            word_count_map[word] = word_count_map[word] + int(count)\n",
    "\n",
    "    #recreate bag of words from word counts to be able to use countvectorizer\n",
    "    text = []\n",
    "    for word, count in word_count_map.items():\n",
    "        transformed_word = stemmer.stem(word) #comment this out, and uncomment the following to proceed w/o stemming\n",
    "        #transformed_word = word\n",
    "        word_list = [transformed_word] * count\n",
    "        text.extend(word_list)\n",
    "    text = ' '.join(text) #convert list to string\n",
    "    text = ''.join([i for i in text if not i.isdigit()]) #remove digits from string\n",
    "        \n",
    "    n_tup = (n_id, text)\n",
    "    n_list.append(n_tup) \n",
    "    #add list to dataframe at the end of each file\n",
    "df_n = pd.DataFrame(n_list)\n",
    "df_n.columns = ['n_id', 'body']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort rows by ngram id and save to csv\n",
    "CSV will be imported in notebooks 3 and 4 and matched against metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_n.sort_values(by='n_id')\n",
    "df_n.to_csv('output/df-n.csv', encoding='utf-8', index=True, header=True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note\n",
    "We are still working with ngrams for all 8,808 articles in the corpus here (pre-metadata cleaning). \n",
    "We ignore articles that aren't included in the df-R-cleaned.csv (metadata) list though in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df_n) == filesystem_article_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Go to notebook 3 >>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
