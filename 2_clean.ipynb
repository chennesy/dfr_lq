{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries for importing and parsing jstor dfr files\n",
    "import pandas as pd\n",
    "import glob \n",
    "import re\n",
    "import numpy as np\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the metadata from R\n",
    "df_R = pd.read_csv('part1_journal_article_jst_get_article-1.csv', encoding='utf-8', na_filter=False)\n",
    "df_R.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how many articles are in the metadata file?\n",
    "filesystem_article_count = len(glob.glob(\"ngrams/*.txt\"))\n",
    "starting_article_count = len(df_R)\n",
    "assert starting_article_count == filesystem_article_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove \"articles\" of front and back matter from corpus\n",
    "#drop all rows with article_title of \"Front Matter\", \"The Cover Design\", \"The Cover\", \"Back Matter\", \"Volume Information\"\n",
    "\n",
    "def row_count_for_column_value(df_R, column, value):\n",
    "    return len(df_R[df_R[column] == value])\n",
    "\n",
    "def row_count_for_column_value_regex(df_R, column, regex):\n",
    "    return len(df_R[df_R[column].str.contains(regex, regex=True, na=False)])\n",
    "\n",
    "for article_title in ['Front Matter','Back Matter','Volume Information',]:\n",
    "    if row_count_for_column_value(df_R, 'article_title', article_title) == 0:\n",
    "        print(f'no articles with title \"{article_title}\"')\n",
    "        continue\n",
    "    df_R = df_R[~(df_R['article_title']==article_title)]\n",
    "    assert row_count_for_column_value(df_R, 'article_title', article_title) == 0\n",
    "    \n",
    "for article_title_regex in ['^The\\sCover','^Cover\\sDesign',]:\n",
    "    if row_count_for_column_value_regex(df_R, 'article_title', article_title_regex) == 0:\n",
    "        print(f'no articles with title \"{article_title}\"')\n",
    "        continue\n",
    "    df_R = df_R[~(df_R['article_title'].str.contains(article_title_regex, regex=True, na=False))]\n",
    "    assert row_count_for_column_value_regex(df_R, 'article_title', article_title_regex) == 0\n",
    "    \n",
    "# Add output for number of articles removed?\n",
    "len(df_R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#move journal_pub_id values for 'libraryq' into journal_jcode column - these were incorrectly mapped via the R import\n",
    "#df_R['journal_pub_id'].unique()\n",
    "journal_pub_id_count = row_count_for_column_value(df_R, 'journal_pub_id', 'libraryq') # 608\n",
    "starting_journal_jcode_count = row_count_for_column_value(df_R, 'journal_jcode', 'libraryq') # 7477\n",
    "df_R.loc[df_R['journal_pub_id']=='libraryq', 'journal_jcode'] = 'libraryq'\n",
    "assert row_count_for_column_value(df_R, 'journal_jcode', 'libraryq') == starting_journal_jcode_count + journal_pub_id_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create parent journal id by replacing former title codes\n",
    "df_R['jid_combined'] = df_R['journal_jcode']\n",
    "starting_libraryq_count = row_count_for_column_value(df_R, 'jid_combined', 'libraryq') # 8085\n",
    "starting_LQ_count = row_count_for_column_value(df_R, 'jid_combined', 'LQ') # 0 Is this what we expect? May be a holdover from a larger corpus, so can probably remove.\n",
    "# print(starting_LQ_count) # Though this is 0, leaving this cell here for now, since other code uses the jid_combined column.\n",
    "df_R['jid_combined'] = df_R['jid_combined'].str.replace('LQ', 'libraryq', regex=False)\n",
    "assert row_count_for_column_value(df_R, 'jid_combined', 'libraryq') == starting_libraryq_count + starting_LQ_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create id column to match against ngram ids\n",
    "#len(df_R['file_name'].str.contains(r\"^journal-article-\", regex=True, na=False)) # 8099\n",
    "assert row_count_for_column_value_regex(df_R, 'file_name', r'^journal-article-') > 0\n",
    "df_R['file_name'] = df_R['file_name'].str.replace(r'^journal-article-', '', regex=True)\n",
    "assert row_count_for_column_value_regex(df_R, 'file_name', r'^journal-article-') == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export to csv for notebook 2 combination with ngrams\n",
    "df_R.to_csv('output/df-R-cleaned.csv', encoding='utf-8', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "nltk.download('stopwords')\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weird/inefficient idea here: I'm reconstituting bag of words from word counts to be able to use count vectorizer\n",
    "n_list = []\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "\n",
    "ngrams = glob.iglob(\"ngrams/*.txt\")\n",
    "\n",
    "for ngram in ngrams:\n",
    "    word_count_map = {}\n",
    "    #create n_id from file name\n",
    "    n_id = ngram[:-11].strip('ngrams/journal-article-') \n",
    "    with open(ngram) as csv_ng:\n",
    "        csvReader = csv.reader(csv_ng, delimiter='\\t')\n",
    "        for row in csvReader:\n",
    "            word, count = row[0], row[1]\n",
    "            if word not in word_count_map:\n",
    "                word_count_map[word] = 0\n",
    "            word_count_map[word] = word_count_map[word] + int(count)\n",
    "\n",
    "    #recreate bag of words from word counts to be able to use wordvectorizer\n",
    "    text = []\n",
    "    for word, count in word_count_map.items():\n",
    "        transformed_word = stemmer.stem(word) #comment this out, and uncomment the following to proceed w/o stemming\n",
    "        #transformed_word = word\n",
    "        word_list = [transformed_word] * count\n",
    "        text.extend(word_list)\n",
    "    text = ' '.join(text) #convert list to string\n",
    "    text = ''.join([i for i in text if not i.isdigit()]) #remove digits from string\n",
    "        \n",
    "    n_tup = (n_id, text)\n",
    "    n_list.append(n_tup) \n",
    "    #add list to dataframe at the end of each file\n",
    "df_n = pd.DataFrame(n_list)\n",
    "df_n.columns = ['n_id', 'body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort by id and save to csv\n",
    "df_n.sort_values(by='n_id')\n",
    "df_n.to_csv('output/df-n.csv', encoding='utf-8', index=True, header=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#note we created word vectors for all 8,808 articles in the corpus here. \n",
    "#we'll ignore those that aren't included in the df-R-cleaned.csv (metadata) list though in the next step\n",
    "assert len(df_n) == filesystem_article_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## STOP HERE and GO TO part 3 ##"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
