{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries for topic modeling\n",
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "import csv\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "import pickle\n",
    "import xml.etree.ElementTree as ET\n",
    "import glob\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import CSVs\n",
    "* Import df-sparse-cleaned.csv and df-n.csv from the output of `1_tm_import_clean.ipynb`. (This notebook doesn't seem to exist anymore. Is this why the filesystem paths below don't match those in `2_clean.ipynb`?)\n",
    "* Then combine both into a new dataframe to use in this session, 'df_all', matching on n_id and id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_R = pd.read_csv('output/df-R-cleaned.csv', encoding='utf-8', na_filter=False) #article metadata\n",
    "df_n = pd.read_csv('output/df-n.csv', encoding='utf-8') #article ngrams\n",
    "df_all = df_R.merge(df_n, left_on='file_name', right_on='n_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#total number of records to analyze:\n",
    "assert len(df_R) < len(df_n) # In 2_clean.ipynb, we removed some articles from df_R that we did not remove from df_n.\n",
    "assert len(df_all) == len(df_R) # Ensure we are excluding the records we previously removed from df_R.\n",
    "len(df_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus metadata\n",
    "Run the numbers here on the basic outlines of the corpus; number of articles per year, per journal, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the number of articles by year\n",
    "year_count = df_R.groupby(['pub_year']).count()[['file_name']]\n",
    "year_count.columns = ['article_count']\n",
    "year_count.to_csv('output/articles_per_year.csv', encoding='utf-8', index=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot articles by year\n",
    "x_year = year_count.index\n",
    "y_count = year_count['article_count']\n",
    "plt.plot(x_year, y_count)\n",
    "plt.ylabel(\"Number of articles\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.title('LQ Corpus: Number of articles per year')\n",
    "#plt.show()\n",
    "plt.savefig('output/lq_tm/plots/art_per_year.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#articles per issue\n",
    "journals = df_R.groupby(['pub_year', 'volume', 'issue']).count()[['file_name']]\n",
    "journals.columns = ['article_count']\n",
    "journals = journals.sort_values('pub_year', ascending=True)\n",
    "journals\n",
    "journals.to_csv('output/articles_per_issue.csv', encoding='utf-8', index=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of entries = number of journal issues\n",
    "journals.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean articles per issue:', journals.article_count.mean()) # cody's number: 22.728070175438596 my number: 23.681286549707604"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#articles per year\n",
    "journals = df_R.groupby(['pub_year']).count()[['file_name']]\n",
    "journals.columns = ['article_count']\n",
    "journals = journals.sort_values('pub_year', ascending=True)\n",
    "journals.article_count.mean() # cody's number: 91.44705882352942 my number: 95.28235294117647\n",
    "# Why is this commented-out line here?\n",
    "#journals.to_csv('output/background/articles_per_year.csv', encoding='utf-8', index=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_articles_issue = journals.groupby(['pub_year']).mean()[['article_count']]\n",
    "#mean_articles_issue.to_csv('output/background/mean_articles_per_issue.csv', encoding='utf-8', index=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_year = mean_articles_issue.index\n",
    "y_count = mean_articles_issue['article_count']\n",
    "plt.plot(x_year, y_count)\n",
    "plt.ylabel(\"Articles per issue\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.title('LQ Corpus: Mean articles per issue')\n",
    "#plt.show()\n",
    "plt.savefig('output/lq_tm/plots/mean_art_per_issue.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count by type of article\n",
    "type_count = df_R.groupby(['article_type']).count()[['file_name']]\n",
    "type_count.columns = ['type_count']\n",
    "type_count = type_count.sort_values('type_count',ascending=False)\n",
    "type_count.to_csv('output/article_types.csv', encoding='utf-8', index=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#articles per journal title ## Not relevant for LQ study\n",
    "journals_t = df_R.groupby(['journal_title']).count()[['file_name']]\n",
    "journals_t.columns = ['journal_count']\n",
    "journals_t = journals_t.sort_values('journal_count', ascending=False)\n",
    "journals_t.to_csv('output/articles_per_journal_title.csv', encoding='utf-8', index=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#overall wordcount \n",
    "word_count = df_all['body'].apply(lambda x: len(str(x).split()))\n",
    "word_count.sum() # cody's number: 16614346 my number: 11556509 This discrepancy is likely due to past over-counting in notebook 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer & LDA Topic Model\n",
    "* Convert wordlists from df_all.body to Term Frequency vector. \n",
    "* tf = term frequency vector\n",
    "* lda = latent dirichlet allocation; fit using tf model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=0.70, min_df=0.10,\n",
    "                                max_features=None)\n",
    "\n",
    "tf = tf_vectorizer.fit_transform(df_all.body.values.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list stop words cut off by max_ and min_df thresholds: \n",
    "# max_df removes words that appear in more than 70% of articles and min_df removes words that appear in fewer than 10%\n",
    "# max_df removes 24 words\n",
    "# min_df removes 145,639  words\n",
    "with open('output/stop_words_all.txt', 'w') as f:\n",
    "    for item in tf_vectorizer.stop_words_:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA\n",
    "This cell currently takes ~15 min (Which cell? The only one that took a long time for me was the one where we write lda_model.pk. --naughton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 40 # set the number of topics based on GridSearchCV best model NOTE: n_topics is deprecated\n",
    "\n",
    "print(\"Fitting LDA models with tf features, \"\n",
    "      \"n_components=%d...\"\n",
    "      % (n_components))\n",
    "\n",
    "#define the lda function, with desired options\n",
    "lda = LatentDirichletAllocation(n_components=n_components, max_iter=20, # number of iterations receommended by GredSearchCV best model ()\n",
    "                                learning_method='online',\n",
    "                                random_state=0)\n",
    "#fit the model\n",
    "lda.fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to save model\n",
    "# This took a long time, and almost all of my cpu! - naughton\n",
    "with open('output/lda_model.pk', 'wb') as pickle_file:\n",
    "    pickle.dump(lda, pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reload lda model from here when restarting notebook\n",
    "with open('output/lda_model.pk', 'rb') as pickle_file:\n",
    "    lda = pickle.load(pickle_file)\n",
    "# then reload it with\n",
    "#lda = pickle.load('output/lda_model.pk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Log Likelihood: \", lda.score(tf))\n",
    "# Cody's number: -65649964.560244985 my number: -45566599.934905805"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Perplexity: \", lda.perplexity(tf))\n",
    "# Cody's number: 744.4134338720977 my number: 658.6748073402453"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top words per topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to return the top words for each topic\n",
    "n_top_words = 30 # how many words per topic\n",
    "topic_word_list = []\n",
    "def return_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        #topic_n = \"\\nTopic #%d:\" % topic_idx)\n",
    "        top_words = \" \".join([feature_names[i]\n",
    "                     for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        topic_word_list.append(top_words) \n",
    "    topic_df = pd.DataFrame(topic_word_list)\n",
    "    topic_df.to_csv('output/topic_words_lq_only.csv', encoding='utf-8', index=True, header=True)\n",
    "    return(topic_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`topic_word_list[x]` will return the top 30 words for each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the top words per topic, using the function defined above.\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "return_top_words(lda, tf_feature_names, n_top_words) # Since we don't use return_top_words anywhere else, I presume this is just a sanity check or something. --naughton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most representative articles per topic\n",
    "`article_list[x]` will return the top 10 articles aligned with each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_dist = lda.transform(tf)\n",
    "topic_dist_df = pd.DataFrame(topic_dist)\n",
    "df_w_topics = topic_dist_df.join(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_list = []\n",
    "for d in range(n_components): # What is d? Just in index? But it seems we're also using it as a 'prevalence' column, maybe incorrectly. --naughton\n",
    "    tup = df_w_topics[['article_jcode', 'article_title', 'article_type', 'journal_title', 'jid_combined', 'pub_year', d]].sort_values(by=[d], ascending=False)[0:10]\n",
    "    #print(f'{d}: {tup}')\n",
    "    article_list.append(tup)\n",
    "    article_list[d].columns = ['article_jcode', 'article_title', 'article_type', 'journal_title', 'jid_combined', 'pub_year', 'prevalence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add topic column\n",
    "for n, i in enumerate(article_list):\n",
    "    i['topic'] = n\n",
    "article_lists_df = pd.concat(article_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#article_lists_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find book review titles in jstor xml since it didn't come through via step 1 (R) and append to top articles list\n",
    "review_list = []\n",
    "for index, row in article_lists_df[article_lists_df['article_title']==''].iterrows():\n",
    "    #insert jstor code to pull book name and author from XML\n",
    "    review_list.append(row['article_jcode'])\n",
    "reviews_df_list = []\n",
    "for xml_file in glob.iglob(\"metadata/*.xml\"):\n",
    "    jid = re.search(r'(\\d*).xml$', xml_file).group(1) # jid (formerly xid) is the jstor code, it seems\n",
    "    if jid not in review_list:\n",
    "        continue\n",
    "    tree = ET.parse(xml_file)\n",
    "    for book_reviewed_node in tree.getroot().findall('./front/article-meta/product/source'):\n",
    "        tup = (jid, book_reviewed_node.text)\n",
    "        print(tup)\n",
    "        reviews_df_list.append(tup)\n",
    "\n",
    "reviews_df = pd.DataFrame(reviews_df_list, columns=['jid', 'book_reviewed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_lists_df = article_lists_df.merge(reviews_df, how='outer', left_on='article_jcode', right_on='jid')\n",
    "article_lists_df = article_lists_df.drop(['jid'], axis=1)\n",
    "article_lists_df.loc[article_lists_df['article_type']=='book-review', 'article_title'] = article_lists_df['book_reviewed']\n",
    "article_lists_df = article_lists_df.drop(['book_reviewed'], axis=1)\n",
    "article_lists_df = article_lists_df.sort_values(by=['topic', 'prevalence'], ascending=[True, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove full dupe rows (there was one introduced for the article \"Party Girl\")\n",
    "article_lists_df.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_lists_df.to_csv('output/top_articles_per_topic_lq.csv', encoding='utf-8', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print output to multiple csv files\n",
    "#OPTIONAL - deprecated by full article_lists_df above\n",
    "for index, df in enumerate(article_list):\n",
    "    filename = 'output/articles_per_topic/top_articles_' + str(index) + '.csv'\n",
    "    df.to_csv(filename, encoding='utf-8', index=True, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w_topics['word_count'] = df_w_topics['body'].apply(lambda x: len(str(x).split()))\n",
    "#df_w_topics['word_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multiple topic weight by word count\n",
    "col_list = []\n",
    "for num in range(n_components): # This was originally called topic_columns. Not sure why yet... --naughton\n",
    "    col = \"%d_wc\" % num\n",
    "    col_list.append(col)\n",
    "    df_w_topics[col] = df_w_topics[num] * df_w_topics['word_count']\n",
    "#df_w_topics[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the prevalence of each topic\n",
    "prevalence = []\n",
    "for index, e in enumerate(col_list): # Why 'e'? --naughton\n",
    "    prev = df_w_topics[e].sum()/df_w_topics['word_count'].sum()\n",
    "    tup =(index,prev)\n",
    "    prevalence.append(tup)\n",
    "prevalence = pd.DataFrame(prevalence)\n",
    "prevalence.columns = ['topic','prevalence']\n",
    "prevalence.sort_values(by=['prevalence'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prevalence.to_csv('output/prevalence_per_topic_lq.csv', encoding='utf-8', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_year = df_w_topics.groupby('pub_year')\n",
    "fig3 = plt.figure()\n",
    "# divide the number of topic words in each year by the total word count per year (so the figure adjusts to each year's output)\n",
    "for e in col_list:\n",
    "    ax2 = fig3.add_subplot(1,1,1)\n",
    "    (grouped_year[e].sum()/grouped_year['word_count'].sum()).plot(kind='line', title=e)\n",
    "    fig3.tight_layout()\n",
    "    #plt.show()\n",
    "    filename = 'output/plots/plot_' + str(e) + '.png'\n",
    "    plt.savefig(filename)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document term matrix\n",
    "Create document-term-matrix dataframe, dtm_df, to look at the following (ignoring topic models):\n",
    "* most common words in corpus\n",
    "* average number of times each word is used in an article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_df = pd.DataFrame(tf_vectorizer.fit_transform(df_all.body.values.astype('U')).toarray(), columns=tf_vectorizer.get_feature_names(), index = df_all.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find most common words in corpus + avg times each is used in an article \n",
    "most_common_words = dtm_df.sum().sort_values(ascending=False)[0:500]\n",
    "avg_times_used = dtm_df.mean().sort_values(ascending=False)[0:500]\n",
    "df_top = pd.DataFrame(most_common_words)\n",
    "df_top.columns = ['word_count']\n",
    "df_top['avg_used'] = avg_times_used\n",
    "df_top.to_csv('output/top_words_lq.csv', encoding='utf-8', index=True, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_10_1 = df_all[df_all['article_jcode'].str.contains('4309398')]\n",
    "topic_10_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alt: Create topic model viz\n",
    "* Use pyLDA vis to visualize alternate topics generated via scikit-learn\n",
    "* outputs to html file for future reference outside of notebook\n",
    "* pyLDAvis is based on LDAvis (for R) and using \"relevance\" method for ranking terms within a topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "p = pyLDAvis.sklearn.prepare(lda, tf, tf_vectorizer)\n",
    "pyLDAvis.save_html(p,'output/lda_tm_lq.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
